---
title: "TwitterTest"
author: "André Calero Valdez"
date: "5/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(lubridate)
```

# Verbindung mit Twitter aufbauen

## Tweets
```{r}
tweets <- rtweet::search_tweets(q = "#Arzt", n = 10000, include_rts = TRUE, lang = "de")

tweets %>% filter(screen_name == "DIEZEIT")

tweets %>% filter(status_id == "1260108474744274945")
```

## User Daten
```{r}
user_data <- rtweet::users_data(tweets)  %>% unique()

user_data %>% arrange(desc(followers_count))

user_data %>% ggplot() +
  aes(x = followers_count) +
  geom_histogram()
```

# Timelines

```{r}
tl <- rtweet::get_timelines(user = c("DIEZEIT", "derspiegel"), n = 1000)


tl %>% group_by(screen_name) %>% 
  summarise(beginn = min(created_at))

tl %>% filter(created_at > dmy("30.04.2020")) %>% 
  group_by(screen_name) %>% ts_plot()

```



```{r}
## Stream keywords used to filter tweets
q <- "covid19,corona,stayhome"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
streamtime <- 60

## Filename to save json data (backup)
filename <- "data/rtelect.json"

## Stream election tweets
rt <- stream_tweets(q = q, timeout = streamtime, file_name = filename, lang="de")
```

```{r}
rt <- parse_stream("data/rtelect.json")

rt %>% filter(lang == "de")
```

# Automatische Textanalyse (Natural Language Processing = NLP)
-Tokenization (in Wörter zerlegen)
-Stopwörter gefiltert
-Häufigsten Wörter angezeigt

```{r}
names(rt)

library(tidytext)

words_from_tweets <- tl %>% 
  filter(lang == "de") %>% 
  select(screen_name, status_id, text, favorite_count) %>% 
  unnest_tokens(word, text)

my_stopwords <- tibble(word = stopwords::stopwords("de")) %>% 
                bind_rows(tibble(word = c("t.co", "https", "mehr", "dass","amp",
                                          "eigentlich", "ab", "viele", "trotz", "19",
                                          "11", "via", "mio", "1")))

words_from_tweets %>% 
  anti_join(my_stopwords) %>% 
  group_by(word, screen_name) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  group_by(screen_name) %>% 
  top_n(20) %>% 
  ggplot() +
  aes(x = reorder(factor(word), n), y = n, fill = screen_name) +
  geom_col() +
  xlab("") +
  coord_flip() +
  facet_wrap(~screen_name, scales = "free")



```


# Social Network Analysis

```{r}
friends <- get_friends("sumidu")

result <- lookup_users(friends$user_id)
```


```{r}

result %>% select(user_id, screen_name, friends_count) %>% arrange(desc(friends_count)) %>% head()


step1 <- friends %>% left_join(result) %>% select(user, screen_name, friends_count) %>% arrange(friends_count) %>% filter(friends_count > 1)

step1 

test <- step1$screen_name %>% head()

fof <- get_friends(test, retryonratelimit = T)

result_fof <- lookup_users(fof$user_id)


list_fof <- fof %>% left_join(result_fof) %>% select(user, screen_name, friends_count) 


step1 <- friends %>% left_join(result) %>% select(user, screen_name, friends_count) 

network <- list_fof %>% bind_rows(step1)



library(igraph)
library(tidygraph)
set.seed(124567) 

network %>% head(40) %>% 
  graph_from_data_frame() %>% 
  as_tbl_graph() %>% 
  ggraph::ggraph(layout = "fr") + 
  ggraph::geom_edge_link() + 
  ggraph::geom_node_point(
    size = 1
    ) +
  ggraph::geom_node_label(mapping = aes(label = name)) + theme_void()

```


```{r}
library(threejs)  
fed <-  network %>% head(40) %>% 
  graph_from_data_frame() %>% 
  as_tbl_graph() %>% 
  tidygraph::as.igraph()  

V(fed)$size <- 1 
graph_attr(fed, "layout") <- NULL  
graphjs(g = fed, bg = "gray10",  
        showLabels = T,  
        layout = list(  
          layout_with_fr(fed, dim = 3)),  
                  main = "")


```

